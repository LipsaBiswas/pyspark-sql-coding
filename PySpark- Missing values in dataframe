Identifying missing values and treating/imputing missing values is an important step in exploratory data analysis. Let's try it in PySpark dataframe API.
Task - In a PySpark dataframe,
a) Identify columns that contain no null values.
b) Identify columns where every value is null.
c) Identify columns with at least one null value

emp_data = [
 (1,'Neha' , 30 , None, 'IT'),
 (2,'Mark' , None , None, 'HR'),
 (3,'David' , 25 , None, 'HR'),
 (4,'Carol' , 30 , None, None)
]

======================================================================================================

from pyspark.sql import SparkSession
from pyspark.sql.types import * 
from pyspark.sql.functions import * 

spark =  SparkSession.builder.appName("myapp").getOrCreate()

emp_data = [
    (1,'Neha' , 30 , None, 'IT'),
    (2,'Mark' ,  None , None, 'HR'),
    (3,'David' , 25 , None, 'HR'),
    (4,'Carol' , 30 , None, None)
]

emp_schema = StructType([
    StructField("Id" , IntegerType()) ,
    StructField("Name" , StringType()) ,
    StructField("Age" , IntegerType()) ,
    StructField("Salary" , IntegerType()) ,
    StructField("Department" , StringType()) ]
)

df =  spark.createDataFrame(data = emp_data , schema =emp_schema  )
df


any_null_value_columns = [ col_name    for col_name in df.columns    if (df.filter( isnull(col(col_name)))).count()!=0]
any_null_value_columns


no_null_value_columns = [ col_name    for col_name in df.columns    if (df.filter( isnull(col(col_name)))).count()==0]
no_null_value_columns




all_null_value_columns = [ col_name    for col_name in df.columns    if (df.filter( (col(col_name)).isNotNull())).count()==0]
all_null_value_columns
